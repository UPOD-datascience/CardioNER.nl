{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:54: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:54: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_1321236/1477342896.py:54: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  sentences = re.split(\"([\\.!\\?]\\s+)\", lines[l_idx])\n",
      "/tmp/ipykernel_1321236/1477342896.py:59: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  words = re.split(\"(\\s+)\", sentences[s_idx])\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaModel, BertModel, PreTrainedTokenizer\n",
    "from tokenizers import Encoding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric, F1Score, Precision, Recall, Accuracy\n",
    "from torch import Tensor\n",
    "from torch.nn import ModuleDict\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# \n",
    "def flatten_list(l):\n",
    "    if not isinstance(l, list):\n",
    "        return [l]\n",
    "    return [item for sublist in l for item in flatten_list(sublist)]\n",
    "\n",
    "\n",
    "def align_labels_to_text(text_encoding: Encoding, labels: list[dict], tag2label: dict):\n",
    "    num_labels = len(tag2label.keys())\n",
    "    text_labels = torch.zeros((text_encoding.input_ids.shape[1], num_labels))\n",
    "    for label in labels:\n",
    "        tag, start_idx, end_idx = label[\"tag\"], int(label[\"start_span\"]), int(label[\"end_span\"])\n",
    "        start_token_idx = text_encoding.char_to_token(start_idx)\n",
    "        end_token_idx = text_encoding.char_to_token(end_idx - 1) + 1\n",
    "        text_labels[start_token_idx:end_token_idx, tag2label[tag]] = 1\n",
    "    text_labels[~text_labels[:, 1:].any(dim=1), 0] = 1  # Adding null class if no other label is present\n",
    "    return text_labels\n",
    "\n",
    "\n",
    "def split_text(text: str, tokenizer: PreTrainedTokenizer, max_seq_len: int):\n",
    "    paragraphs = re.split(\"(\\n\\n)\", text)\n",
    "    paragraphs = [\"\".join(paragraphs[i : i + 2]) for i in range(0, len(paragraphs), 2)]\n",
    "    for p_idx in range(len(paragraphs)):\n",
    "        ids = tokenizer.encode(paragraphs[p_idx], add_special_tokens=True)\n",
    "        if len(ids) > max_seq_len:\n",
    "            lines = re.split((\"(\\n)\"), paragraphs[p_idx])\n",
    "            lines = [\"\".join(lines[i : i + 2]) for i in range(0, len(lines), 2)]\n",
    "            for l_idx in range(len(lines)):\n",
    "                ids = tokenizer.encode(lines[l_idx], add_special_tokens=True)\n",
    "                if len(ids) > max_seq_len:\n",
    "                    sentences = re.split(\"([\\.!\\?]\\s+)\", lines[l_idx])\n",
    "                    sentences = [\"\".join(sentences[i : i + 2]) for i in range(0, len(sentences), 2)]\n",
    "                    for s_idx in range(len(sentences)):\n",
    "                        ids = tokenizer.encode(sentences[s_idx], add_special_tokens=True)\n",
    "                        if len(ids) > max_seq_len:\n",
    "                            words = re.split(\"(\\s+)\", sentences[s_idx])\n",
    "                            words = [\"\".join(words[i : i + 2]) for i in range(0, len(words), 2)]\n",
    "                            sentences[s_idx] = words\n",
    "                    lines[l_idx] = sentences\n",
    "            paragraphs[p_idx] = lines\n",
    "    splits = flatten_list(paragraphs)\n",
    "    return splits\n",
    "\n",
    "\n",
    "def get_tokens_indices(char_to_token_list: list[int], start_idx: int, end_idx: int):\n",
    "    token_idx_list = [char_to_token_list[i] for i in range(start_idx, end_idx) if char_to_token_list[i] is not None]\n",
    "    token_idx_list = [k for k, _ in itertools.groupby(token_idx_list)]\n",
    "    return token_idx_list\n",
    "\n",
    "\n",
    "def merge_splits_into_chunks(\n",
    "    text: str,\n",
    "    splits: list[str],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_seq_len: int,\n",
    "    labels: list[dict],\n",
    "    tag2label: dict,\n",
    "):\n",
    "    encoding = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    char_to_token_list = [encoding.char_to_token(i) for i in range(len(text))]\n",
    "    text_ids = encoding.input_ids[0]\n",
    "    text_label_ids = align_labels_to_text(encoding, labels, tag2label)\n",
    "    num_labels = len(tag2label.keys())\n",
    "    assert len(text_ids) == len(text_label_ids)\n",
    "\n",
    "    # Merge splits into chunks without exceeding max_seq_len\n",
    "    start_chunk_idx, end_chunk_idx = 0, 0\n",
    "    chunks = {\"text\": [], \"input_ids\": [], \"label_ids\": []}\n",
    "    for i in range(len(splits) + 1):\n",
    "        # TODO: optimize this\n",
    "        if i < len(splits):\n",
    "            # Compute the current chunk length after adding the next tokenized split\n",
    "            sentence = splits[i]\n",
    "            token_idx_list = get_tokens_indices(char_to_token_list, start_chunk_idx, end_chunk_idx + len(sentence))\n",
    "            chunk_ids = text_ids[token_idx_list]\n",
    "        if i == len(splits) or len(chunk_ids) > max_seq_len - 2:  # account for [CLS] and [SEP] token\n",
    "            # add previous splits as a chunk if current chunk exceeds max_seq_len - 2 or if the splits are finished\n",
    "            token_idx_list = get_tokens_indices(char_to_token_list, start_chunk_idx, end_chunk_idx)\n",
    "            chunk_ids = torch.cat(\n",
    "                [\n",
    "                    torch.LongTensor([tokenizer.cls_token_id]),\n",
    "                    text_ids[token_idx_list],\n",
    "                    torch.LongTensor([tokenizer.sep_token_id]),\n",
    "                ]\n",
    "            )\n",
    "            chunk_labels_ids = torch.cat(\n",
    "                [\n",
    "                    torch.LongTensor([[-100] * num_labels]),\n",
    "                    text_label_ids[token_idx_list],\n",
    "                    torch.LongTensor([[-100] * num_labels]),\n",
    "                ],\n",
    "            )\n",
    "            chunks[\"text\"].append(text[start_chunk_idx:end_chunk_idx])\n",
    "            chunks[\"input_ids\"].append(chunk_ids)\n",
    "            chunks[\"label_ids\"].append(chunk_labels_ids)\n",
    "            start_chunk_idx = end_chunk_idx\n",
    "        end_chunk_idx += len(sentence)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardioCCC(Dataset):\n",
    "    LABEL_FOLDERS = [\"dis\", \"med\", \"symp\", \"proc\"]\n",
    "\n",
    "    def __init__(self, root_path: str, split: str, lang: str = \"it\"):\n",
    "        self.root_path = Path(root_path)\n",
    "        self.split_file_names = json.load((self.root_path / \"splits.json\").open())[lang][split][\"symp\"]\n",
    "        self.lang = lang\n",
    "        batches = [\"b1\", \"b2\"] if lang != \"ro\" else [\"b1\"]\n",
    "        self.annotations = []\n",
    "        for batch in batches:\n",
    "            lang_path = self.root_path / batch / \"1_validated_without_sugs\" / lang\n",
    "            raw_annotations = []\n",
    "            for label_folder in self.LABEL_FOLDERS:\n",
    "                ann_path = lang_path / label_folder / \"tsv\"\n",
    "                raw_annotations.append(pd.read_csv(next(ann_path.glob(\"*.tsv\")), sep=\"\\t\", na_filter=False))\n",
    "            raw_annotations = pd.concat(raw_annotations, axis=0)\n",
    "\n",
    "            for group in raw_annotations.groupby(\"name\"):\n",
    "                if group[0] not in self.split_file_names:\n",
    "                    continue\n",
    "                file_name = group[0] + \".txt\"\n",
    "                text = (lang_path / \"dis/txt\" / file_name).read_text()\n",
    "                labels = group[1].loc[:, [\"tag\", \"start_span\", \"end_span\", \"text\"]].to_dict(orient=\"records\")\n",
    "                self.annotations.append({\"text\": text, \"labels\": labels})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "\n",
    "\n",
    "class ChunkedCardioCCC(Dataset):\n",
    "    TAG2LABEL = {\"0\": 0, \"DISEASE\": 1, \"MEDICATION\": 2, \"PROCEDURE\": 3, \"SYMPTOM\": 4}\n",
    "    LABEL2TAG = {v: k for k, v in TAG2LABEL.items()}\n",
    "\n",
    "    def __init__(self, dataset: CardioCCC, tokenizer: PreTrainedTokenizer, language: str, iter_by_chunk: bool = False, model_max_len: int = 512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.language = language\n",
    "        self.chunked_data = []\n",
    "        self.iter_by_chunk = iter_by_chunk\n",
    "        for i, item in enumerate(dataset):\n",
    "            text, labels = item[\"text\"], item[\"labels\"]\n",
    "            splits = split_text(text, tokenizer, model_max_len)\n",
    "            chunks = merge_splits_into_chunks(text, splits, tokenizer, model_max_len, labels, self.TAG2LABEL)\n",
    "            if iter_by_chunk:\n",
    "                for i in range(len(chunks[\"text\"])):\n",
    "                    self.chunked_data.append(\n",
    "                        {\n",
    "                            \"text\": chunks[\"text\"][i],\n",
    "                            \"input_ids\": chunks[\"input_ids\"][i],\n",
    "                            \"label_ids\": chunks[\"label_ids\"][i],\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                self.chunked_data.append(chunks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunked_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.chunked_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_chunked_bert(batch: list[dict]):\n",
    "    input_ids = [chunk[\"input_ids\"] for chunk in batch]\n",
    "    labels = [chunk[\"label_ids\"] for chunk in batch]\n",
    "    attention_mask = [torch.ones_like(ids) for ids in input_ids]\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "class NEREval(Metric):\n",
    "    def __init__(self, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"labels\", default=[], dist_reduce_fx=\"cat\")\n",
    "        metric_classes_dict = {\"f1\": F1Score, \"precision\": Precision, \"recall\": Recall, \"accuracy\": Accuracy}\n",
    "        self.classification_metrics = ModuleDict(\n",
    "            {\n",
    "                k\n",
    "                + (f\"_{avg}\" if avg != \"none\" else \"\"): v(task=\"multilabel\", num_labels=num_labels, average=avg, zero_division=1)\n",
    "                for k, v in metric_classes_dict.items()\n",
    "                for avg in [\"none\", \"micro\", \"macro\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def update(self, preds: Tensor, labels: Tensor) -> None:\n",
    "        self.preds.append(preds)\n",
    "        self.labels.append(labels)\n",
    "\n",
    "    def compute(self):\n",
    "        preds, labels = self.preds, self.labels\n",
    "        if isinstance(preds, list):\n",
    "            preds, labels = torch.cat(self.preds), torch.cat(self.labels)\n",
    "\n",
    "        results = {}\n",
    "        for metric_name, metric in self.classification_metrics.items():\n",
    "            results[metric_name] = metric(preds, labels)\n",
    "            metric.reset()\n",
    "        return results\n",
    "\n",
    "\n",
    "class NERModule(L.LightningModule):\n",
    "    def __init__(self, lm: nn.Module, lm_output_size: int, label2tag: int):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "        self.lm_output_size = lm_output_size\n",
    "        self.label2tag = label2tag\n",
    "        self.num_labels = len(label2tag.keys())\n",
    "        self.classifier = nn.Linear(lm_output_size, self.num_labels)\n",
    "        self.metric = NEREval(num_labels=self.num_labels)\n",
    "\n",
    "    def exclude_padding_and_special_tokens(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        logits = logits.view(-1, self.num_labels)\n",
    "        labels = labels.view(-1, self.num_labels)\n",
    "        label_mask = labels[:, 0] != -100  # exclude padding and special tokens\n",
    "        logits = logits[label_mask]\n",
    "        labels = labels[label_mask]\n",
    "        return logits, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        sequence_out = self.lm(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        logits = self.classifier(sequence_out)\n",
    "        logits, labels = self.exclude_padding_and_special_tokens(logits, labels)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        sequence_out = self.lm(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        logits = self.classifier(sequence_out)\n",
    "        logits, labels = self.exclude_padding_and_special_tokens(logits, labels)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, sync_dist=True)\n",
    "        preds = logits.sigmoid()\n",
    "        self.metric.update(preds, labels)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        results = self.metric.compute()\n",
    "        for k, v in results.items():\n",
    "            if \"micro\" not in k and \"macro\" not in k:\n",
    "                for i in range(self.num_labels):\n",
    "                    self.log(f\"val_{k}_class_{self.label2tag[i]}\", v[i], on_epoch=True, sync_dist=True)\n",
    "            else:\n",
    "                self.log(f\"val_{k}\", v, on_epoch=True, sync_dist=True)\n",
    "        self.metric.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        sequence_out = self.lm(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        logits = self.classifier(sequence_out)\n",
    "        logits, labels = self.exclude_padding_and_special_tokens(logits, labels)\n",
    "        preds = logits.sigmoid()\n",
    "        self.metric.update(preds, labels)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        results = self.metric.compute()\n",
    "        new_results = {}\n",
    "        for k, v in results.items():\n",
    "            if \"micro\" not in k and \"macro\" not in k:\n",
    "                for i in range(self.num_labels):\n",
    "                    new_results[f\"test_{k}_class_{self.label2tag[i]}\"] = v[i].item()\n",
    "                    self.log(f\"test_{k}_class_{self.label2tag[i]}\", v[i], on_epoch=True, sync_dist=True)\n",
    "            else:\n",
    "                new_results[f\"test_{k}\"] = v.item()\n",
    "                self.log(f\"test_{k}\", v, on_epoch=True, sync_dist=True)\n",
    "        self.metric.reset()\n",
    "        return new_results\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 512). Running this sequence through the model will result in indexing errors\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type      | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | lm         | BertModel | 109 M  | eval \n",
      "1 | classifier | Linear    | 3.8 K  | train\n",
      "2 | metric     | NEREval   | 0      | train\n",
      "-------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.364   Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "225       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2e9aabdbc646229a3a63058feb7b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0311af2b9b71442a9958aaa27c6395d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4732baab2244f0b3858c906e91b061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1806cb845ec64123addd2c7c507a3c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962e12349d9e43c3ba2a2b1bf67cd6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3b258072cc4c6595738df07e4742ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa12109106f844f1bd0c01b876b0cee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfcc4984dec4ad1a54f32a37a11a6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55986d34b8c4060be1cdf349b2b2d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5c4b42a0ea4d6a8606acf6008de45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">           Test metric           </span>┃<span style=\"font-weight: bold\">          DataLoader 0           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_accuracy_class_0      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.8890659809112549        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_accuracy_class_DISEASE   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9163823127746582        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_accuracy_class_MEDICATION  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.8887782692909241        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_accuracy_class_PROCEDURE  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9110364317893982        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_accuracy_class_SYMPTOM   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9301469326019287        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy_macro       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9070820212364197        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy_micro       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9070819616317749        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_f1_class_0         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9201626777648926        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_f1_class_DISEASE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9480224251747131        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_f1_class_MEDICATION     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9222813844680786        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_f1_class_PROCEDURE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9430197477340698        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_f1_class_SYMPTOM      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9561691284179688        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1_macro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9379310607910156        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1_micro          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9387432932853699        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_precision_class_0      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        0.925544261932373        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_precision_class_DISEASE   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9501193761825562        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_precision_class_MEDICATION </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9267457127571106        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_precision_class_PROCEDURE  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9432283043861389        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_precision_class_SYMPTOM   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        0.960542619228363        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision_macro       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9412360191345215        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision_micro       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9419880509376526        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_recall_class_0       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9148433208465576        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_recall_class_DISEASE    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9459347724914551        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_recall_class_MEDICATION   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9178598523139954        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_recall_class_PROCEDURE   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9428112506866455        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_recall_class_SYMPTOM    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9518352746963501        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall_macro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9346568584442139        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall_micro        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9355207681655884        </span>│\n",
       "└─────────────────────────────────┴─────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m          Test metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m         DataLoader 0          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_accuracy_class_0     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.8890659809112549       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_accuracy_class_DISEASE  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9163823127746582       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_accuracy_class_MEDICATION \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.8887782692909241       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_accuracy_class_PROCEDURE \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9110364317893982       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_accuracy_class_SYMPTOM  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9301469326019287       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy_macro      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9070820212364197       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy_micro      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9070819616317749       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_f1_class_0        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9201626777648926       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_f1_class_DISEASE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9480224251747131       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_f1_class_MEDICATION    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9222813844680786       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_f1_class_PROCEDURE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9430197477340698       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_f1_class_SYMPTOM     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9561691284179688       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1_macro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9379310607910156       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1_micro         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9387432932853699       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_precision_class_0     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       0.925544261932373       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_precision_class_DISEASE  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9501193761825562       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_precision_class_MEDICATION\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9267457127571106       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_precision_class_PROCEDURE \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9432283043861389       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_precision_class_SYMPTOM  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       0.960542619228363       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision_macro      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9412360191345215       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision_micro      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9419880509376526       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_recall_class_0      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9148433208465576       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_recall_class_DISEASE   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9459347724914551       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_recall_class_MEDICATION  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9178598523139954       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_recall_class_PROCEDURE  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9428112506866455       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_recall_class_SYMPTOM   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9518352746963501       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall_macro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9346568584442139       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall_micro       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9355207681655884       \u001b[0m\u001b[35m \u001b[0m│\n",
       "└─────────────────────────────────┴─────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_f1_class_0': 0.9201626777648926,\n",
       "  'test_f1_class_DISEASE': 0.9480224251747131,\n",
       "  'test_f1_class_MEDICATION': 0.9222813844680786,\n",
       "  'test_f1_class_PROCEDURE': 0.9430197477340698,\n",
       "  'test_f1_class_SYMPTOM': 0.9561691284179688,\n",
       "  'test_f1_micro': 0.9387432932853699,\n",
       "  'test_f1_macro': 0.9379310607910156,\n",
       "  'test_precision_class_0': 0.925544261932373,\n",
       "  'test_precision_class_DISEASE': 0.9501193761825562,\n",
       "  'test_precision_class_MEDICATION': 0.9267457127571106,\n",
       "  'test_precision_class_PROCEDURE': 0.9432283043861389,\n",
       "  'test_precision_class_SYMPTOM': 0.960542619228363,\n",
       "  'test_precision_micro': 0.9419880509376526,\n",
       "  'test_precision_macro': 0.9412360191345215,\n",
       "  'test_recall_class_0': 0.9148433208465576,\n",
       "  'test_recall_class_DISEASE': 0.9459347724914551,\n",
       "  'test_recall_class_MEDICATION': 0.9178598523139954,\n",
       "  'test_recall_class_PROCEDURE': 0.9428112506866455,\n",
       "  'test_recall_class_SYMPTOM': 0.9518352746963501,\n",
       "  'test_recall_micro': 0.9355207681655884,\n",
       "  'test_recall_macro': 0.9346568584442139,\n",
       "  'test_accuracy_class_0': 0.8890659809112549,\n",
       "  'test_accuracy_class_DISEASE': 0.9163823127746582,\n",
       "  'test_accuracy_class_MEDICATION': 0.8887782692909241,\n",
       "  'test_accuracy_class_PROCEDURE': 0.9110364317893982,\n",
       "  'test_accuracy_class_SYMPTOM': 0.9301469326019287,\n",
       "  'test_accuracy_micro': 0.9070819616317749,\n",
       "  'test_accuracy_macro': 0.9070820212364197}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "patience = 5\n",
    "num_workers = 4\n",
    "max_epochs = 30\n",
    "num_labels = len(ChunkedCardioCCC.TAG2LABEL.keys())\n",
    "root_path = \"../assets\"\n",
    "lang = \"it\"\n",
    "model_name = 'IVN-RIN/bioBIT' # \"CLTL/MedRoBERTa.nl\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, add_pooling_layer=False)\n",
    "\n",
    "\n",
    "\n",
    "train = CardioCCC(root_path, \"train\", lang)\n",
    "val = CardioCCC(root_path, \"validation\", lang)\n",
    "test = CardioCCC(root_path, \"test\", lang)\n",
    "train = ChunkedCardioCCC(train, tokenizer, lang, iter_by_chunk=True, model_max_len=model.config.max_position_embeddings)\n",
    "val = ChunkedCardioCCC(val, tokenizer, lang, iter_by_chunk=True,  model_max_len=model.config.max_position_embeddings)\n",
    "test = ChunkedCardioCCC(test, tokenizer, lang, iter_by_chunk=True)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, collate_fn=collate_fn_chunked_bert, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, collate_fn=collate_fn_chunked_bert, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, collate_fn=collate_fn_chunked_bert, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "module = NERModule(lm=model, lm_output_size=model.config.hidden_size, label2tag=train.LABEL2TAG)\n",
    "trainer = L.Trainer(max_epochs=1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience),\n",
    "    ModelCheckpoint(monitor=\"val_loss\", mode=\"min\"),\n",
    "]\n",
    "devices = [0]\n",
    "strategy = \"ddp_find_unused_parameters_true\" if len(devices) > 1 else \"auto\"\n",
    "trainer = L.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    devices=devices,\n",
    "    max_epochs=max_epochs,\n",
    "    strategy=strategy,\n",
    "    precision=\"16-mixed\" if isinstance(devices, list) or devices == \"cuda\" else \"auto\",\n",
    ")\n",
    "trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "trainer.test(model=module, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardioner-nl-e2JFinJK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
