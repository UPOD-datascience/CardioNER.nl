{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:54: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:54: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\bes3\\AppData\\Local\\Temp\\ipykernel_15524\\3694842190.py:54: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  sentences = re.split(\"([\\.!\\?]\\s+)\", lines[l_idx])\n",
      "C:\\Users\\bes3\\AppData\\Local\\Temp\\ipykernel_15524\\3694842190.py:59: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  words = re.split(\"(\\s+)\", sentences[s_idx])\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, RobertaModel, BertModel, PreTrainedTokenizer\n",
    "from tokenizers import Encoding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric, F1Score, Precision, Recall, Accuracy\n",
    "from torch import Tensor\n",
    "from torch.nn import ModuleDict\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# \n",
    "def flatten_list(l):\n",
    "    if not isinstance(l, list):\n",
    "        return [l]\n",
    "    return [item for sublist in l for item in flatten_list(sublist)]\n",
    "\n",
    "\n",
    "def align_labels_to_text(text_encoding: Encoding, labels: list[dict], tag2label: dict):\n",
    "    num_labels = len(tag2label.keys())\n",
    "    text_labels = torch.zeros((text_encoding.input_ids.shape[1], num_labels))\n",
    "    for label in labels:\n",
    "        tag, start_idx, end_idx = label[\"tag\"], int(label[\"start_span\"]), int(label[\"end_span\"])\n",
    "        start_token_idx = text_encoding.char_to_token(start_idx)\n",
    "        end_token_idx = text_encoding.char_to_token(end_idx - 1)\n",
    "        text_labels[start_token_idx:end_token_idx, tag2label[tag]] = 1\n",
    "    text_labels[~text_labels[:, 1:].any(dim=1)] = 1  # Adding null class if no other label is present\n",
    "    return text_labels\n",
    "\n",
    "\n",
    "def split_text(text: str, tokenizer: PreTrainedTokenizer, max_seq_len: int):\n",
    "    paragraphs = re.split(\"(\\n\\n)\", text)\n",
    "    paragraphs = [\"\".join(paragraphs[i : i + 2]) for i in range(0, len(paragraphs), 2)]\n",
    "    for p_idx in range(len(paragraphs)):\n",
    "        ids = tokenizer.encode(paragraphs[p_idx], add_special_tokens=True)\n",
    "        if len(ids) > max_seq_len:\n",
    "            lines = re.split((\"(\\n)\"), paragraphs[p_idx])\n",
    "            lines = [\"\".join(lines[i : i + 2]) for i in range(0, len(lines), 2)]\n",
    "            for l_idx in range(len(lines)):\n",
    "                ids = tokenizer.encode(lines[l_idx], add_special_tokens=True)\n",
    "                if len(ids) > max_seq_len:\n",
    "                    sentences = re.split(\"([\\.!\\?]\\s+)\", lines[l_idx])\n",
    "                    sentences = [\"\".join(sentences[i : i + 2]) for i in range(0, len(sentences), 2)]\n",
    "                    for s_idx in range(len(sentences)):\n",
    "                        ids = tokenizer.encode(sentences[s_idx], add_special_tokens=True)\n",
    "                        if len(ids) > max_seq_len:\n",
    "                            words = re.split(\"(\\s+)\", sentences[s_idx])\n",
    "                            words = [\"\".join(words[i : i + 2]) for i in range(0, len(words), 2)]\n",
    "                            sentences[s_idx] = words\n",
    "                    lines[l_idx] = sentences\n",
    "            paragraphs[p_idx] = lines\n",
    "    splits = flatten_list(paragraphs)\n",
    "    return splits\n",
    "\n",
    "\n",
    "def get_tokens_indices(char_to_token_list: list[int], start_idx: int, end_idx: int):\n",
    "    token_idx_list = [char_to_token_list[i] for i in range(start_idx, end_idx) if char_to_token_list[i] is not None]\n",
    "    token_idx_list = [k for k, _ in itertools.groupby(token_idx_list)]\n",
    "    return token_idx_list\n",
    "\n",
    "\n",
    "def merge_splits_into_chunks(\n",
    "    text: str,\n",
    "    splits: list[str],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_seq_len: int,\n",
    "    labels: list[dict],\n",
    "    tag2label: dict,\n",
    "):\n",
    "    encoding = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    char_to_token_list = [encoding.char_to_token(i) for i in range(len(text))]\n",
    "    text_ids = encoding.input_ids[0]\n",
    "    text_label_ids = align_labels_to_text(encoding, labels, tag2label)\n",
    "    num_labels = len(tag2label.keys())\n",
    "    assert len(text_ids) == len(text_label_ids)\n",
    "\n",
    "    # Merge splits into chunks without exceeding max_seq_len\n",
    "    start_chunk_idx, end_chunk_idx = 0, 0\n",
    "    chunks = {\"text\": [], \"input_ids\": [], \"label_ids\": []}\n",
    "    for i in range(len(splits) + 1):\n",
    "        # TODO: optimize this\n",
    "        if i < len(splits):\n",
    "            # Compute the current chunk length after adding the next tokenized split\n",
    "            sentence = splits[i]\n",
    "            token_idx_list = get_tokens_indices(char_to_token_list, start_chunk_idx, end_chunk_idx + len(sentence))\n",
    "            chunk_ids = text_ids[token_idx_list]\n",
    "        if i == len(splits) or len(chunk_ids) > max_seq_len - 2:  # account for [CLS] and [SEP] token\n",
    "            # add previous splits as a chunk if current chunk exceeds max_seq_len - 2 or if the splits are finished\n",
    "            token_idx_list = get_tokens_indices(char_to_token_list, start_chunk_idx, end_chunk_idx)\n",
    "            chunk_ids = torch.cat(\n",
    "                [\n",
    "                    torch.LongTensor([tokenizer.cls_token_id]),\n",
    "                    text_ids[token_idx_list],\n",
    "                    torch.LongTensor([tokenizer.sep_token_id]),\n",
    "                ]\n",
    "            )\n",
    "            chunk_labels_ids = torch.cat(\n",
    "                [\n",
    "                    torch.LongTensor([[-100] * num_labels]),\n",
    "                    text_label_ids[token_idx_list],\n",
    "                    torch.LongTensor([[-100] * num_labels]),\n",
    "                ],\n",
    "            )\n",
    "            chunks[\"text\"].append(text[start_chunk_idx:end_chunk_idx])\n",
    "            chunks[\"input_ids\"].append(chunk_ids)\n",
    "            chunks[\"label_ids\"].append(chunk_labels_ids)\n",
    "            start_chunk_idx = end_chunk_idx\n",
    "        end_chunk_idx += len(sentence)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardioCCC(Dataset):\n",
    "    LABEL_FOLDERS = [\"dis\", \"med\", \"symp\", \"proc\"]\n",
    "\n",
    "    def __init__(self, root_path: str, split: str, lang: str = \"it\", encoding: str = 'latin-1'):\n",
    "        self.root_path = Path(root_path)\n",
    "        self.split_file_names = json.load((self.root_path / \"splits.json\").open())[lang][split][\"symp\"]\n",
    "        self.lang = lang\n",
    "        batches = [\"b1\", \"b2\"] if lang != \"ro\" else [\"b1\"]\n",
    "        self.annotations = []\n",
    "        for batch in batches:\n",
    "            lang_path = self.root_path / batch / \"1_validated_without_sugs\" / lang\n",
    "            raw_annotations = []\n",
    "            for label_folder in self.LABEL_FOLDERS:\n",
    "                ann_path = lang_path / label_folder / \"tsv\"\n",
    "                raw_annotations.append(pd.read_csv(next(ann_path.glob(\"*.tsv\")), sep=\"\\t\", na_filter=False))\n",
    "            raw_annotations = pd.concat(raw_annotations, axis=0)\n",
    "\n",
    "            for group in raw_annotations.groupby(\"name\"):\n",
    "                if group[0] not in self.split_file_names:\n",
    "                    continue\n",
    "                file_name = group[0] + \".txt\"\n",
    "                text = (lang_path / \"dis/txt\" / file_name).read_text(encoding=encoding)\n",
    "                labels = group[1].loc[:, [\"tag\", \"start_span\", \"end_span\", \"text\"]].to_dict(orient=\"records\")\n",
    "                self.annotations.append({\"text\": text, \"labels\": labels})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "\n",
    "\n",
    "class ChunkedCardioCCC(Dataset):\n",
    "    TAG2LABEL = {\"0\": 0, \"DISEASE\": 1, \"MEDICATION\": 2, \"PROCEDURE\": 3, \"SYMPTOM\": 4}\n",
    "    LABEL2TAG = {v: k for k, v in TAG2LABEL.items()}\n",
    "\n",
    "    def __init__(self, dataset: CardioCCC, tokenizer: PreTrainedTokenizer, language: str, iter_by_chunk: bool = False, model_max_len: int = 512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.language = language\n",
    "        self.chunked_data = []\n",
    "        self.iter_by_chunk = iter_by_chunk\n",
    "        for i, item in enumerate(dataset):\n",
    "            text, labels = item[\"text\"], item[\"labels\"]\n",
    "            splits = split_text(text, tokenizer, model_max_len)\n",
    "            chunks = merge_splits_into_chunks(text, splits, tokenizer, model_max_len, labels, self.TAG2LABEL)\n",
    "            if iter_by_chunk:\n",
    "                for i in range(len(chunks[\"text\"])):\n",
    "                    self.chunked_data.append(\n",
    "                        {\n",
    "                            \"text\": chunks[\"text\"][i],\n",
    "                            \"input_ids\": chunks[\"input_ids\"][i],\n",
    "                            \"label_ids\": chunks[\"label_ids\"][i],\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                self.chunked_data.append(chunks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunked_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.chunked_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_chunked_bert(batch: list[dict]):\n",
    "    input_ids = [chunk[\"input_ids\"] for chunk in batch]\n",
    "    labels = [chunk[\"label_ids\"] for chunk in batch]\n",
    "    attention_mask = [torch.ones_like(ids) for ids in input_ids]\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "class NEREval(Metric):\n",
    "    def __init__(self, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"labels\", default=[], dist_reduce_fx=\"cat\")\n",
    "        metric_classes_dict = {\"f1\": F1Score, \"precision\": Precision, \"recall\": Recall, \"accuracy\": Accuracy}\n",
    "        self.classification_metrics = ModuleDict(\n",
    "            {\n",
    "                k\n",
    "                + (f\"_{avg}\" if avg != \"none\" else \"\"): v(task=\"multilabel\", num_labels=num_labels, average=avg, zero_division=1)\n",
    "                for k, v in metric_classes_dict.items()\n",
    "                for avg in [\"none\", \"micro\", \"macro\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def update(self, preds: Tensor, labels: Tensor) -> None:\n",
    "        self.preds.append(preds)\n",
    "        self.labels.append(labels)\n",
    "\n",
    "    def compute(self):\n",
    "        preds, labels = self.preds, self.labels\n",
    "        if isinstance(preds, list):\n",
    "            preds, labels = torch.cat(self.preds), torch.cat(self.labels)\n",
    "\n",
    "        results = {}\n",
    "        for metric_name, metric in self.classification_metrics.items():\n",
    "            results[metric_name] = metric(preds, labels)\n",
    "            metric.reset()\n",
    "        return results\n",
    "\n",
    "\n",
    "class NERModule(L.LightningModule):\n",
    "    def __init__(self, lm: nn.Module, lm_output_size: int, label2tag: int):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "        self.lm_output_size = lm_output_size\n",
    "        self.label2tag = label2tag\n",
    "        self.num_labels = len(label2tag.keys())\n",
    "        self.classifier = nn.Linear(lm_output_size, self.num_labels)\n",
    "        self.metric = NEREval(num_labels=self.num_labels)\n",
    "\n",
    "    def exclude_padding_and_special_tokens(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        logits = logits.view(-1, self.num_labels)\n",
    "        labels = labels.view(-1, self.num_labels)\n",
    "        label_mask = labels[:, 0] != -100  # exclude padding and special tokens\n",
    "        logits = logits[label_mask]\n",
    "        labels = labels[label_mask]\n",
    "        return logits, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        sequence_out = self.lm(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        logits = self.classifier(sequence_out)\n",
    "        logits, labels = self.exclude_padding_and_special_tokens(logits, labels)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        sequence_out = self.lm(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        logits = self.classifier(sequence_out)\n",
    "        logits, labels = self.exclude_padding_and_special_tokens(logits, labels)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, sync_dist=True)\n",
    "        preds = logits.sigmoid()\n",
    "        self.metric.update(preds, labels)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        results = self.metric.compute()\n",
    "        for k, v in results.items():\n",
    "            if \"micro\" not in k and \"macro\" not in k:\n",
    "                for i in range(self.num_labels):\n",
    "                    self.log(f\"val_{k}_class_{self.label2tag[i]}\", v[i], on_epoch=True, sync_dist=True)\n",
    "            else:\n",
    "                self.log(f\"val_{k}\", v, on_epoch=True, sync_dist=True)\n",
    "        self.metric.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        sequence_out = self.lm(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        logits = self.classifier(sequence_out)\n",
    "        logits, labels = self.exclude_padding_and_special_tokens(logits, labels)\n",
    "        preds = logits.sigmoid()\n",
    "        self.metric.update(preds, labels)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        results = self.metric.compute()\n",
    "        new_results = {}\n",
    "        for k, v in results.items():\n",
    "            if \"micro\" not in k and \"macro\" not in k:\n",
    "                for i in range(self.num_labels):\n",
    "                    new_results[f\"test_{k}_class_{self.label2tag[i]}\"] = v[i].item()\n",
    "                    self.log(f\"test_{k}_class_{self.label2tag[i]}\", v[i], on_epoch=True, sync_dist=True)\n",
    "            else:\n",
    "                new_results[f\"test_{k}\"] = v.item()\n",
    "                self.log(f\"test_{k}\", v, on_epoch=True, sync_dist=True)\n",
    "        self.metric.reset()\n",
    "        return new_results\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length: 514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\bes3\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cardioner-nl-o7dqlUGo-py3.12\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose a notebook-compatible strategy: `Trainer(strategy='ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mddp_find_unused_parameters_true\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m     40\u001b[0m strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cpu \u001b[38;5;28;01melse\u001b[39;00m strategy \u001b[38;5;66;03m# ddp_spawn if use_cpu and not in notebook\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cpu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m16-mixed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbf16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(module, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_loader, val_dataloaders\u001b[38;5;241m=\u001b[39mval_loader)\n\u001b[0;32m     50\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model\u001b[38;5;241m=\u001b[39mmodule, dataloaders\u001b[38;5;241m=\u001b[39mtest_loader)\n",
      "File \u001b[1;32mc:\\Users\\bes3\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cardioner-nl-o7dqlUGo-py3.12\\Lib\\site-packages\\lightning\\pytorch\\utilities\\argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bes3\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cardioner-nl-o7dqlUGo-py3.12\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:396\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43m_AcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bes3\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cardioner-nl-o7dqlUGo-py3.12\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py:163\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[1;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_init_precision()\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# 6. Instantiate Strategy - Part 2\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bes3\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cardioner-nl-o7dqlUGo-py3.12\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py:581\u001b[0m, in \u001b[0;36m_AcceleratorConnector._lazy_init_strategy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_configure_launcher()\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mis_interactive_compatible:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer(strategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy_flag\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m)` is not compatible with an interactive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m environment. Run your code as a script, or choose a notebook-compatible strategy:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer(strategy=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddp_notebook\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m In case you are spawning processes yourself, make sure to include the Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m creation inside the worker function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    587\u001b[0m     )\n\u001b[0;32m    589\u001b[0m \u001b[38;5;66;03m# TODO: should be moved to _check_strategy_and_fallback().\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# Current test check precision first, so keep this check here to meet error order\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, XLAAccelerator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, (SingleDeviceXLAStrategy, XLAStrategy)\n\u001b[0;32m    593\u001b[0m ):\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose a notebook-compatible strategy: `Trainer(strategy='ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "patience = 5\n",
    "num_workers = 4\n",
    "max_epochs = 30\n",
    "num_labels = len(ChunkedCardioCCC.TAG2LABEL.keys())\n",
    "root_path = \"T://laupodteam/AIOS/Bram/notebooks/code_dev/CardioNER.nl/assets\"\n",
    "lang = \"nl\"\n",
    "model_name = \"CLTL/MedRoBERTa.nl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name, add_pooling_layer=False)\n",
    "devices = [4] #[0]\n",
    "use_cpu = True\n",
    "\n",
    "max_len = model.config.max_position_embeddings\n",
    "\n",
    "print(f\"The maximum length: {max_len}\")\n",
    "\n",
    "train = CardioCCC(root_path, \"train\", lang)\n",
    "val = CardioCCC(root_path, \"validation\", lang)\n",
    "test = CardioCCC(root_path, \"test\", lang)\n",
    "train = ChunkedCardioCCC(train, tokenizer, lang, iter_by_chunk=True, model_max_len=max_len)\n",
    "val = ChunkedCardioCCC(val, tokenizer, lang, iter_by_chunk=True,  model_max_len=max_len)\n",
    "test = ChunkedCardioCCC(test, tokenizer, lang, iter_by_chunk=True)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, collate_fn=collate_fn_chunked_bert, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, collate_fn=collate_fn_chunked_bert, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, collate_fn=collate_fn_chunked_bert, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "module = NERModule(lm=model, lm_output_size=model.config.hidden_size, label2tag=train.LABEL2TAG)\n",
    "trainer = L.Trainer(max_epochs=1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience),\n",
    "    ModelCheckpoint(monitor=\"val_loss\", mode=\"min\"),\n",
    "]\n",
    "strategy = \"ddp_find_unused_parameters_true\" if len(devices) > 1 else \"auto\" \n",
    "strategy = 'ddp' if use_cpu else strategy # ddp_spawn if use_cpu and not in notebook\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    devices=devices[0] if use_cpu else devices,\n",
    "    max_epochs=max_epochs,\n",
    "    strategy=strategy,\n",
    "    precision=\"16-mixed\" if isinstance(devices, list) or devices == \"cuda\" else \"bf16\",\n",
    ")\n",
    "trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "trainer.test(model=module, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardioner-nl-o7dqlUGo-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
